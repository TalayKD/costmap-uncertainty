INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (723) bind mounts
INFO: WorkFlow created.
INFO: working_dir: ./, exp_prefix: t_8_, lr: 0.001, lr_decay: True, batch_size: 200, test_batch_size: 200, worker_num: 8, train_step: 5000, snapshot: 1250, image_width: 224, image_height: 224, hsv_rand: 0.0, rand_blur: 0.0, data_file: combine_train_crop10.txt, val_file: combine_test_crop10.txt, load_model: False, model_name: , test: False, test_traj: False, test_num: 10, test_interval: 100, print_interval: 1, plot_interval: 100, network: 2, net_config: 0, no_data_augment: False, multi_gpu: 1, platform: local, stride: 3, skip: 1, crop_num: 10, data_root: /project/learningphysics, test_vel: 0.0, out_vid_file: , finetune: False, 
INFO: AVP initialized.
INFO: t_8 #1 - loss: 0.4992  lr: 0.001000 - time load/bp (21.40, 10.97)
INFO: t_8 #2 - loss: 0.4827  lr: 0.001000 - time load/bp (0.03, 0.89)
INFO: t_8 #3 - loss: 0.3879  lr: 0.001000 - time load/bp (0.03, 1.71)
INFO: t_8 #4 - loss: 0.3509  lr: 0.001000 - time load/bp (0.00, 0.67)
INFO: t_8 #5 - loss: 0.2883  lr: 0.001000 - time load/bp (0.00, 0.70)
INFO: t_8 #6 - loss: 0.2626  lr: 0.001000 - time load/bp (0.00, 0.86)
INFO: t_8 #7 - loss: 0.1748  lr: 0.001000 - time load/bp (0.00, 1.06)
INFO: t_8 #8 - loss: 0.2002  lr: 0.001000 - time load/bp (0.00, 0.84)
INFO: t_8 #9 - loss: 0.1953  lr: 0.001000 - time load/bp (62.76, 1.13)
INFO: t_8 #10 - loss: 0.1768  lr: 0.001000 - time load/bp (0.00, 0.53)
INFO: t_8 #11 - loss: 0.1606  lr: 0.001000 - time load/bp (0.00, 1.70)
INFO: t_8 #12 - loss: 0.1935  lr: 0.001000 - time load/bp (0.00, 1.13)
INFO: t_8 #13 - loss: 0.1741  lr: 0.001000 - time load/bp (0.04, 0.80)
INFO: t_8 #14 - loss: 0.1436  lr: 0.001000 - time load/bp (0.00, 0.64)
INFO: t_8 #15 - loss: 0.1630  lr: 0.001000 - time load/bp (0.01, 0.59)
INFO: t_8 #16 - loss: 0.1476  lr: 0.001000 - time load/bp (0.01, 0.80)
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py", line 1133, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/usr/lib/python3.8/multiprocessing/queues.py", line 107, in get
    if not self._poll(timeout):
  File "/usr/lib/python3.8/multiprocessing/connection.py", line 257, in poll
    return self._poll(timeout)
  File "/usr/lib/python3.8/multiprocessing/connection.py", line 424, in _poll
    r = wait([self], timeout)
  File "/usr/lib/python3.8/multiprocessing/connection.py", line 931, in wait
    ready = selector.select(timeout)
  File "/usr/lib/python3.8/selectors.py", line 415, in select
    fd_event_list = self._selector.poll(timeout)
  File "/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 4071039) is killed by signal: Killed. 

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/data2/datasets/tkondhor/code/ss_costmap/train_costmap_wf.py", line 493, in <module>
    trainCostmap.train()
  File "/data2/datasets/tkondhor/code/ss_costmap/train_costmap_wf.py", line 270, in train
    sample = next(self.trainDataiter)
  File "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py", line 634, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py", line 1329, in _next_data
    idx, data = self._get_data()
  File "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py", line 1295, in _get_data
    success, data = self._try_get_data()
  File "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py", line 1146, in _try_get_data
    raise RuntimeError('DataLoader worker (pid(s) {}) exited unexpectedly'.format(pids_str)) from e
RuntimeError: DataLoader worker (pid(s) 4071039) exited unexpectedly
slurmstepd: error: Detected 1 oom-kill event(s) in StepId=1955.batch cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler.
